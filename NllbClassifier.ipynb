{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zlkhyr/NllbNusaX/blob/main/NllbClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2X9fcQl2Omy"
      },
      "outputs": [],
      "source": [
        "# !pip install datasets\n",
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBpVpCryt9_w"
      },
      "source": [
        "#Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCkbCPb8c4ff"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from torch.optim import AdamW\n",
        "from google.colab import files\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "# from datasets import load_dataset, concatenate_datasets\n",
        "from transformers import (\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EboxJCbat7BN"
      },
      "source": [
        "#Load Nllb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dx88K7arKlTg"
      },
      "outputs": [],
      "source": [
        "model_name = \"facebook/nllb-200-distilled-600M\"\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "MYQIUohyuFy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsjjyeGOsUS7"
      },
      "source": [
        "#test tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoP_S9ED3ar5"
      },
      "outputs": [],
      "source": [
        "token = tokenizer.encode('ini adalah contoh teks' )\n",
        "token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nVExwhR34BR"
      },
      "outputs": [],
      "source": [
        "tokenizer.convert_ids_to_tokens(token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpPMmNjBuyQM"
      },
      "source": [
        "#Data Preporocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyeSbeZ0D7CL"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/Final_Year/data_train.csv')\n",
        "val = pd.read_csv('/content/drive/MyDrive/Final_Year/data_val.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/Final_Year/data_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doaIC8Nnv_5k"
      },
      "outputs": [],
      "source": [
        "train.info(), test.info(), val.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHIFETJy0oSO"
      },
      "outputs": [],
      "source": [
        "# map_bahasa ={\n",
        "#   'aceh' : 'ace_Latn',\n",
        "#   'bali' : 'ban_Latn',\n",
        "#   'banjar' : 'bjn_Latn',\n",
        "#   'bugis' : 'bug_Latn',\n",
        "#   'indonesia' : 'ind_Latn',\n",
        "#   'inggris' : 'eng_Latn',\n",
        "#   'jawa' : 'jav_Latn',\n",
        "#   'minang' : 'min_Latn',\n",
        "#   'sunda' : 'sun_Latn',\n",
        "\n",
        "#   'batak toba' : 'ind_Latn',\n",
        "#   'madura' : 'ind_Latn',\n",
        "#   'ngaju' : 'ind_Latn'\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Y7hzSXLF2Fm"
      },
      "outputs": [],
      "source": [
        "def preprocess(dataset, shuffle):\n",
        "\n",
        "  label_map = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
        "  dataset[\"label\"] = dataset[\"label\"].map(label_map)\n",
        "\n",
        "  # tokenizer.src_lang = 'ind_Latn'\n",
        "\n",
        "  encoding = tokenizer(\n",
        "      dataset['text'].tolist(),\n",
        "      padding=True,\n",
        "      truncation=True,\n",
        "      max_length=model.config.max_length,\n",
        "      return_tensors=\"pt\"\n",
        "  )\n",
        "\n",
        "  labels = torch.tensor(dataset['label'].tolist(), dtype=torch.long)\n",
        "  dataset = TensorDataset(encoding['input_ids'], encoding['attention_mask'], labels)\n",
        "  dataloader = DataLoader(dataset, batch_size=32, shuffle=shuffle)\n",
        "  return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUjC6jgDEltc"
      },
      "outputs": [],
      "source": [
        "train_dataloader = preprocess(train, True)\n",
        "val_dataloader = preprocess(val, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP_KrBWNtq9B"
      },
      "source": [
        "#Pengembangan Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7b4z6QXC2DV"
      },
      "source": [
        "##Model NllbClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiAqtUpyg2dv"
      },
      "outputs": [],
      "source": [
        "class NllbClassifier(nn.Module):\n",
        "    def __init__(self, encoder, num_labels=3):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder #encoder dari NLLB model\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(encoder.config.hidden_size, num_labels)\n",
        "\n",
        "    def mean_pooling(self, model_output, attention_mask):\n",
        "        token_embeddings = model_output.last_hidden_state\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        #Menghapus spesial token di awal tiap text input (language token)\n",
        "        input_ids = input_ids[:, 1:]\n",
        "        attention_mask = attention_mask[:, 1:]\n",
        "\n",
        "        #Forward pass ke encoder\n",
        "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # Pooling dengan attention_mask\n",
        "        pooled_output = self.mean_pooling(outputs, attention_mask)\n",
        "\n",
        "        # Dropout layer\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "\n",
        "        # Classification layer\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJgf3aRfDN0H"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckGqL1E5hz8o"
      },
      "outputs": [],
      "source": [
        "encoder = model.get_encoder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mocN0ZyWqron"
      },
      "outputs": [],
      "source": [
        "encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BV08uVhqJt6r"
      },
      "outputs": [],
      "source": [
        "modelFT = NllbClassifier(encoder, num_labels=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AN-MBvGBGevJ"
      },
      "outputs": [],
      "source": [
        "sum(p.numel() for p in modelFT.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HmHwYuZLX6y"
      },
      "outputs": [],
      "source": [
        "modelFT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7EINjb2LS4A"
      },
      "source": [
        "#Freezing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06ihP_pQHB2r"
      },
      "outputs": [],
      "source": [
        "for param in modelFT.encoder.embed_tokens.parameters():\n",
        "  param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq5qiJsPcB8f"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnBmLXAeE8Uu"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Code running di: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxTL5VFJCHTk"
      },
      "outputs": [],
      "source": [
        "modelFT.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "summary(modelFT)"
      ],
      "metadata": {
        "id": "znpJLrRa2rS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(modelFT)"
      ],
      "metadata": {
        "id": "cnqGZOWI3Ma8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQI4OEcD415N"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(modelFT.parameters(), lr=2e-5)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3jhAW7q6CZp"
      },
      "outputs": [],
      "source": [
        "history = {\n",
        "        'train_loss':[],\n",
        "        'val_loss':[],\n",
        "        'train_acc':[],\n",
        "        'val_acc':[]\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nYRQRiAvJBO"
      },
      "outputs": [],
      "source": [
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "        modelFT.train()\n",
        "        total_train_loss = 0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{epochs} [Train]\", leave=False):\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = modelFT(input_ids, attention_mask)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "        train_accuracy = correct_train / total_train\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['train_acc'].append(train_accuracy)\n",
        "\n",
        "        modelFT.eval()\n",
        "        total_val_loss = 0\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_dataloader, desc=f\"Epoch {epoch + 1}/{epochs} [Validation]\", leave=False):\n",
        "                input_ids, attention_mask, labels = batch\n",
        "                input_ids = input_ids.to(device)\n",
        "                attention_mask = attention_mask.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = modelFT(input_ids, attention_mask)\n",
        "                loss = loss_fn(outputs, labels)\n",
        "\n",
        "                total_val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total_val += labels.size(0)\n",
        "                correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
        "        val_accuracy = correct_val / total_val\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "        history['val_acc'].append(val_accuracy)\n",
        "\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss:{avg_train_loss:.4f}, Validation Loss:{avg_val_loss:.4f}, Accuracy:{train_accuracy:.2f}, Validation Accuracy:{val_accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ctn7pwJdb-NM"
      },
      "source": [
        "#Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AknGPAp_YmO7"
      },
      "outputs": [],
      "source": [
        "def learning_curve(history, mode='loss'):\n",
        "  plt.figure()\n",
        "  if mode == 'loss':\n",
        "    plt.plot(history['train_loss'], label='Training Loss')\n",
        "    plt.plot(history['val_loss'], label='Validation Loss')\n",
        "  elif mode == 'acc':\n",
        "    plt.plot(history['train_acc'], label='Training Accuracy')\n",
        "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.title(f'Learning Curve: {mode}')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_curve(history, 'loss'), learning_curve(history, mode='acc')"
      ],
      "metadata": {
        "id": "ErU2MYriQYot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6mCI8XJLIfl"
      },
      "outputs": [],
      "source": [
        "test_dataloader = preprocess(test, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YzIJc3WKwPe"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "  for batch in test_dataloader:\n",
        "    input_ids, attention_mask, labels = batch\n",
        "    input_ids = input_ids.to(device)\n",
        "    attention_mask = attention_mask.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    outputs = modelFT(input_ids, attention_mask)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "    all_preds.extend(predicted.cpu().numpy())\n",
        "    all_labels.extend(labels.cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixXEx2A8fVxD"
      },
      "outputs": [],
      "source": [
        "print(classification_report(all_labels, all_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O57TOOlXkwR1"
      },
      "source": [
        "#test untuk tiap bahasa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYKtf_MVjNdr"
      },
      "outputs": [],
      "source": [
        "test = pd.read_csv('/content/drive/MyDrive/Final_Year/data_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2hFT1POO2vR"
      },
      "outputs": [],
      "source": [
        "test_ing = test[test.bahasa == 'aceh']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpH_7izXU_AQ"
      },
      "outputs": [],
      "source": [
        "test_ing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-t0g-YflUMBK"
      },
      "outputs": [],
      "source": [
        "test_ing_dataloader = preprocess(test_ing, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXZDOosiUBgK"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "  for batch in test_ing_dataloader:\n",
        "    input_ids, attention_mask, labels = batch\n",
        "    input_ids = input_ids.to(device)\n",
        "    attention_mask = attention_mask.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    outputs = modelFT(input_ids, attention_mask)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "    all_preds.extend(predicted.cpu().numpy())\n",
        "    all_labels.extend(labels.cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(all_labels, all_preds))"
      ],
      "metadata": {
        "id": "9WopFcmw0sC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Kw96IHqfs2P"
      },
      "outputs": [],
      "source": [
        "print(classification_report(all_labels, all_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntHM8ZKTniow"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "xsjjyeGOsUS7"
      ],
      "mount_file_id": "15GikkT8rJhjWdHSVjfidm1l0Oa_OfnWL",
      "authorship_tag": "ABX9TyPcKQAmbpyPEXhQmfgYT4Q9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}